{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<font color=#F08080>Ex. 2.1</font>** Suppose each of K-classes has an associated target $t_{k}$, which is a vector of all zeros, except a one in the kth position. Show that classifying to the largest element of $\\hat{y}$ amounts to choosing the closest target, $min_{k}||t_{k} − \\hat{y}||$, if the elements of $\\hat{y}$ sum to one.\n",
    "\n",
    "**<font color=#F08080>Solution:</font>**\n",
    "$$ argmin_{k}||t_{k} − \\hat{y}|| = argmin_{k}||t_{k} − \\hat{y}||^{2} = argmin_{k}\\sum_{i=1}^{K} (t_{k}^{i} - \\hat{y}^{i})^{2}$$\n",
    "\n",
    "$$ = argmin_{k} \\sum_{i=1}^{K} ((t_{k}^{i})^{2} - 2 t_{k}^{i} \\hat{y}^{i} + (\\hat{y}^{i})^{2}) $$\n",
    "\n",
    "where i is the index of a class; $t_{k}^{i}$ is the value of $t_{k}$ at class ith and $\\hat{y}^{i}$ is the value of $\\hat{y}$ at class ith.\n",
    "Notice that, for all class k, $\\sum_{i=1}^{K} (t_{k}^{i})^{2} = 1$ and $\\sum_{i=1}^{K} (\\hat{y}^{i})^{2} = const$ we have:\n",
    "\n",
    "$$ argmin_{k}||t_{k} − \\hat{y}|| = argmin_{k} \\sum_{i=1}^{K} (- 2 t_{k}^{i} \\hat{y}^{i}) = argmin_{k} (- 2 \\hat{y}^{k}) = argmax_{k} (\\hat{y}^{k})$$\n",
    "which is the largest element of $\\hat{y}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<font color=#F08080>Ex. 2.2</font>** Show how to compute the Bayes decision boundary for the simulation example in Figure 2.5.\n",
    "\n",
    "**<font color=#F08080>Solution:</font>** Please refer to [my blog post](https://ethanphan.github.io/datascienceblog/posts/bayes-boundary-with-multivariate-mixture-gaussian-distributions/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<font color=#F08080>Ex. 2.3</font>** Derive equation (2.24):\n",
    "Consider N data points uniformly distributed in a p-dimensional unit ball centered at the origin. Suppose we consider a nearest-neighbor estimate at the origin. The median distance from the origin to the closest data point is given by the expression:\n",
    "$$ d(p, N) = \\left ( 1 - \\frac{1}{2}^{\\frac{1}{N}} \\right )^{\\frac{1}{p}} $$\n",
    "\n",
    "**<font color=#F08080>Solution:</font>**\n",
    "\n",
    "We have volume of a hypersphere in a p-dimension hyperspace given by:\n",
    "\n",
    "$$ V_{n}(r) = \\frac{\\pi^{\\frac{n}{2}}}{\\Gamma(\\frac{n}{2} + 1)}r^{n} $$\n",
    "\n",
    "where where $\\Gamma$ is Leonhard Euler's gamma function. The gamma function extends the factorial function to non-integer arguments. It satisfies $\\Gamma(n) = (n − 1)!$ if n is a positive integer and $\\Gamma(n + \\frac{1}{2}) = (n - \\frac{1}{2})(n - \\frac{3}{2}) ... (\\frac{1}{2}) \\pi^{\\frac{1}{2}}$  if n is a non-negative integer.\n",
    "\n",
    "Since our points are uniformly distributed in a hypersphere with radius R = 1. For each point, the probability of that point lies in a hypersphere with radius r is:\n",
    "\n",
    "$$ P(d_x < r) = \\frac{V_{p}(r)}{V_{p}(R)} = \\frac{r^{p}}{R^{p}} = r^{p} $$\n",
    "\n",
    "Which is the CDF function of $d_x$, where $d_x$ is the distance from point x to the origin of the hyperspace.\n",
    "\n",
    "From there we have the pdf function of $d_x$ is\n",
    "\n",
    "$$ p(d_x) = p d_x^{p-1} $$\n",
    "\n",
    "Let's say $r_m$ is the median distance from the origin to the closest data point which is what we have to estimate. Since $r_m$ is the median, we expect exactly half of the time we draw N points from the distribution, the distance from the origin to closest point of N point is bigger than $r_m$. In other words, whenever we draw N points from the distribution p, the probability of the distances from all N points to the origin is equal to $\\frac{1}{2}$.\n",
    "\n",
    "$$ P(d_x > r_m, \\forall x) = \\prod_{i=1}^{p}p(d_{x_i} > r_m) = \\frac{1}{2} $$\n",
    "\n",
    "$$ => \\prod_{i=1}^{p}(1 - r_{m}^{p}) = \\frac{1}{2} $$\n",
    "\n",
    "$$ => (1 - r_{m}^{p})^{N} = \\frac{1}{2} $$\n",
    "\n",
    "Solve this for $r_{m}$ we have: $ r_{m} = (1 - \\frac{1}{2}^{\\frac{1}{N}})^{\\frac{1}{p}} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<font color=#F08080>Ex. 2.4</font>** The edge effect problem discussed on page 23 is not peculiar to uniform sampling from bounded domains. Consider inputs drawn from a spherical multinormal distribution $X \\sim N(0, I_p)$. The squared distance from any sample point to the origin has a $X_{p}^{2}$ distribution with mean p.\n",
    "Consider a prediction point $x_{0}$ drawn from this distribution, and let $a = \\frac{x_0}{||x_0||}$ be an associated unit vector. Let $z_i = a^{T}x_i$ be the projection of each of the training points on this direction.\n",
    "\n",
    "Show that the $z_i$ are distributed N(0, 1) with expected squared distance from the origin 1, while the target point has expected squared distance p from the origin.\n",
    "\n",
    "Hence for p = 10, a randomly drawn test point is about 3.1 standard deviations from the origin, while all the training points are on average one standard deviation along direction a. So most prediction points see themselves as lying on the edge of the training set.\n",
    "\n",
    "**<font color=#F08080>Solution:</font>**\n",
    "\n",
    "$z = a^{T}x$ means that z is linear combination of multiple random variables $x_{i} \\sim N(0, 1)$ => E|z| = 0.\n",
    "\n",
    "$$ Var(z) = ||a^{T}||^{2} Var(x) = Var(x) = 1$$\n",
    "since $a^{T}$ is an unit vector\n",
    "\n",
    "Square distance from target $x_{0}$ to the origin is:\n",
    "$$ dsquare(x_{0}) = \\sum_{i=1}^{p} (x_{0}^{i})^{2} $$\n",
    "\n",
    "So\n",
    "\n",
    "$$ E[dsquare(x_{0})] = E \\left [ \\sum_{i=1}^{p} (x_{0}^{i})^{2} \\right ] =  \\sum_{i=1}^{p} E[(x_{0}^{i})^{2}]$$\n",
    "\n",
    "$$ = \\sum_{i=1}^{p}(Var(x_{0}^{i}) - E^{2}[x_{0}^{i}] = \\sum_{i=1}^{p}Var(x_{0}^{i})  = p$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<font color=#F08080>Ex. 2.5</font>**\n",
    "\n",
    "(a) Derive equation (2.27). The last line makes use of (3.8) through a conditioning argument.\n",
    "\n",
    "(b) Derive equation (2.28), making use of the cyclic property of the trace operator [trace(AB) = trace(BA)], and its linearity (which allows us to to interchange the order of trace and expectation)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<font color=#F08080>Ex. 2.6</font>** Consider a regression problem with inputs $x_i$ and outputs $y_i$, and a parameterized model $f_{θ}(x)$ to be fit by least squares. Show that if there are observations with tied or identical values of x, then the fit can be obtained from a reduced weighted least squares problem.\n",
    "\n",
    "**<font color=#F08080>Solution:</font>**\n",
    "Let's say we have N data points with M unique input values; M < N , the least square error is\n",
    "\n",
    "$$ RSS = \\sum_{i=1}^{M} \\sum_{j=1}^{N_{i}} (y_{ij} - f_{θ}(x_{ij}))^2$$\n",
    "\n",
    "where $N_i$ is the number of observations of unique input $x_{i}$.\n",
    "\n",
    "$$ RSS = \\sum_{i=1}^{M} \\sum_{j=1}^{N_{i}} (y_{ij}^{2} - 2 y_{ij} f_{θ}(x_ij) + f_{θ}(x_{ij})^2)$$\n",
    "\n",
    "let $\\bar{y_i} = \\frac{1}{N_i} \\sum_{j=1}^{N_i}y_{ij}$ is the mean of all responses y from the same input $x_i$\n",
    "\n",
    "$$ RSS = \\sum_{i=1}^{M} N_{i} ( \\bar{y_{i}}^2 - 2 \\bar{y_{i}} f_{θ}(x_ij) + f_{θ}(x_{i})^2 - \\bar{y_{i}}^2 + \\frac{1}{N_i} \\sum_{j=1}^{N_{i}} y_{ij}^{2} )$$\n",
    "\n",
    "$$ = \\sum_{i=1}^{M} N_{i} ( \\bar{y_{i}} - f_{θ}(x_{i}))^{2} -  \\sum_{i=1}^{M} N_{i} \\bar{y_{i}} + \\sum_{i=1}^{M} \\sum_{j=1}^{N_{i}} y_{ij}^{2}$$\n",
    "\n",
    "Since the observation has already been made, $\\sum_{i=1}^{M} N_{i} \\bar{y_{i}} + \\sum_{i=1}^{M} \\sum_{j=1}^{N_{i}} y_{ij}^{2}$ is fixed, thus minimizing RSS is minimizing\n",
    "\n",
    "$$ \\sum_{i=1}^{M} N_{i} ( \\bar{y_{i}} - f_{θ}(x_{i}))^{2} $$\n",
    "\n",
    "Which is a weighted least squares problem. It's is 'reduced' because now we only have to fit M points instead of N > M points. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<font color=#F08080>Ex. 2.7</font>** Suppose we have a sample of N pairs $x_i, y_i$ drawn i.i.d. from the distribution characterized as follows:\n",
    "- $x_i \\sim h(x)$, the design density\n",
    "- $ y_i = f(x_i) + \\epsilon_i, \\ f$ is the regression function            \n",
    "- $ \\epsilon_i \\sim (0, \\sigma^2) $ (mean, variance $\\sigma^2$)\n",
    "\n",
    "We construct an estimator for $f \\ linear$ in the $y_i$,\n",
    "\n",
    "$$ \\hat{f}(x_0) = \\sum_{i=1}^{N}l_i(x_0; X)y_i$$\n",
    "\n",
    "where the weights $l_i(x_0; X)$ do not depend on $y_i$, but do depend on the entire training sequence of $x_i$, denoted here by $X$.\n",
    "\n",
    "**(a)** Show that linear regression and k-nearest-neighbor regression are mem- bers of this class of estimators. Describe explicitly the weights $l_i(x_0; X)$ in each of these cases.\n",
    "\n",
    "**(b)** Decompose the conditional mean-squared error\n",
    "$$ E_{Y|X} (f(x_0) - \\hat{f}(x_0))^2 $$\n",
    "\n",
    "into a conditional squared bias and a conditional variance component. Like $X, Y$ represents the entire training sequence of $y_i$.\n",
    "\n",
    "**(c)** Decompose the (unconditional) mean-squared error\n",
    "\n",
    "$$ E_{Y,X} (f(x_0) - \\hat{f}(x_0))^2 $$\n",
    "\n",
    "into a squared bias and a variance component.\n",
    "\n",
    "**(d)** Establish a relationship between the squared biases and variances in the above two cases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<font color=#F08080>Solution:</font>**\n",
    "\n",
    "**(a)** The solution for linear regerssion\n",
    "$$ y = f(X) + \\epsilon = X^T \\beta + \\epsilon$$\n",
    "is \n",
    "$$ \\hat{\\beta} = (X^TX)^{-1}X^Ty$$\n",
    "\n",
    "So the estimator for the linear resgression can be written as:\n",
    "$$ \\hat{f}(x_0) = x_0^T((X^TX)^{-1}X^Ty) = \\sum_{i=1}^{N}(x_0^T(X^TX)^{-1}X^T)_i y_i $$ \n",
    "\n",
    "where $(x_0^T(X^TX)^{-1}X^T)_i$ means row ith of the matrix $x_0^T(X^TX)^{-1}X^T$\n",
    "\n",
    "So, in this case: $l_i(x_0; X) = (x_0^T(X^TX)^{-1}X^T)_i$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If $f$ is k-nearest-neighbor regression, we have:\n",
    "\n",
    "$$ \\hat{f}(x_0) = \\frac{1}{k} \\sum_{i=1}^{k} y_i 1_{x_i \\in N_k(x_0)} $$ \n",
    "\n",
    "where $N_k(x_0)$ is the set of k nearest neighbours of $x_0$. We can clearly see that \n",
    "$$ l_i(x_0; X) = \\frac{1}{k} 1_{x_i \\in N_k(x_0)} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(b)** Notice that $x_0$ and $f(x_0)$ are constant, X is fixed and Y varies.\n",
    "\n",
    "$$ MSE = E_{Y|X} ( f(x_0) - \\hat{f}(x_0))^2 = E_{Y|X}( f^2(x_0) - 2 f(x_0)\\hat{f}(x_0) + \\hat{f}^{2}(x_0) ) $$\n",
    "\n",
    "$$ = f^2(x_0) - 2 f(x_0) E_{Y|X} [\\hat{f}(x_0)] + E_{Y|X} [\\hat{f}^{2}(x_0)] $$\n",
    "\n",
    "$$ = f^2(x_0) - 2 f(x_0) E_{Y|X} [\\hat{f}(x_0)] + Var(\\hat{f}(x_0)) - E_{Y|X}^2 [\\hat{f}(x_0)] $$\n",
    "\n",
    "$$ = (f(x_0) - E_{Y|X} [\\hat{f}(x_0)])^2 + Var(\\hat{f}(x_0)) $$\n",
    "$$ = bias^2 + Var(\\hat{f}(x_0))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that: $ \\hat{f}(x_0) = \\sum_{i=1}^{N}l_i(x_0; X)y_i$\n",
    "\n",
    "We can go further (Note that $l_i$ means $l_i(x_0; X)$ for short):\n",
    "$$ E_{Y|X} [\\hat{f}(x_0)] = E_{Y|X} [ \\sum_{i=1}^{N} l_i(f(x_i) + \\epsilon_i) ] = \\sum_{i=1}^{N} l_if(x_i) +  \\sum_{i=1}^{N} l_i E_{Y|X} [ \\epsilon_i ] = \\sum_{i=1}^{N} l_if(x_i) $$\n",
    "\n",
    "$$ E_{Y|X} [\\hat{f}^2(x_0)] = E_{Y|X} [ \\sum_{i=1}^{N} l_i(f(x_i) + \\epsilon_i) \\sum_{j=1}^{N} l_j(f(x_j) + \\epsilon_j) ]$$\n",
    "\n",
    "$$ =  E_{Y|X} [ \\sum_{ij} l_i l_j (f(x_i) + \\epsilon_i)(f(x_j) + \\epsilon_j) ]$$\n",
    "\n",
    "$$ = \\sum_{ij} l_i l_j f(x_i) f(x_j) + E_{Y|X} [ \\sum_{ij} l_i l_j \\epsilon_i f(x_j) ] + E_{Y|X} [\\sum_{ij} l_i l_j \\epsilon_j f(x_i) ] + E_{Y|X} [ \\sum_{ij} l_i l_j \\epsilon_i \\epsilon_j ] $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since $l_i$, $f(x_i)$ are constant, $\\epsilon$ is independent from $l$ $X$ and $f(x)$, and $E[ \\epsilon_i ] = 0$, we have:\n",
    "\n",
    "$$ E_{Y|X} [\\hat{f}^2(x_0)] = \\sum_{ij} l_i l_j f(x_i) f(x_j)  + E_{Y|X} \\left [ \\sum_{ij} l_i l_j \\epsilon_i \\epsilon_j \\right ] $$\n",
    "\n",
    "Note that $\\epsilon_i$ and $\\epsilon_j$ are independent if $i \\neq j$ so:\n",
    "\n",
    "$$ E_{Y|X} [\\hat{f}^2(x_0)] = \\sum_{ij} l_i l_j f(x_i) f(x_j)  + E_{Y|X} \\left [ \\sum_{i=1}^{N} l_{i}^{2} \\epsilon_{i}^{2} \\right ] = \\sum_{ij} l_i l_j f(x_i) f(x_j)  + \\sum_{i=1}^{N} l_{i}^{2} E_{Y|X} [  \\epsilon_{i}^{2} ]$$\n",
    "\n",
    "$$ = \\sum_{ij} l_i l_j f(x_i) f(x_j)  + \\sum_{i=1}^{N} l_{i}^{2}( Var(\\epsilon_{i}) -  E_{Y|X}^{2} [  \\epsilon_{i} ]) = \\sum_{ij} l_i l_j f(x_i) f(x_j)  + \\sigma^{2} \\sum_{i=1}^{N} l_{i}^{2} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have:\n",
    "$$ Bias(\\hat{f}(x_0)) = f(x_0) - E_{Y|X} [\\hat{f}(x_0)]  = f(x_0) - \\sum_{i=1}^{N} l_if(x_i)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And\n",
    "\n",
    "$$ Var(\\hat{f}(x_0)) = \\sum_{ij} l_i l_j f(x_i) f(x_j)  + \\sigma^{2} \\sum_{i=1}^{N} l_{i}^{2} - (\\sum_{i=1}^{N} l_if(x_i))^2 = \\sigma^{2} \\sum_{i=1}^{N} l_{i}^{2} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(c)** Just like in (b), we have\n",
    "$$ MSE = (f(x_0) - E_{Y, X} [\\hat{f}(x_0)])^2 + Var(\\hat{f}(x_0)) = bias^2 + Var(\\hat{f}(x_0))$$\n",
    "\n",
    "But this time, both X and Y varies.\n",
    "\n",
    "$$ E_{Y,X} [\\hat{f}(x_0)] =\n",
    "E_{Y,X} \\left [ \\sum_{i=1}^{N} l_i(f(x_i) + \\epsilon_i) \\right ] =\n",
    "E_{Y,X} \\left [\\sum_{i=1}^{N} l_if(x_i) \\right] +\n",
    "\\sum_{i=1}^{N} E_{Y,X} [l_i] E_{Y,X} [ \\epsilon_i ] =\n",
    "E_{Y,X} \\left [ \\sum_{i=1}^{N} l_if(x_i) \\right ] $$\n",
    "\n",
    "$$ E_{Y,X} [\\hat{f}^2(x_0)] = \n",
    "E_{Y,X} \\left [ \\sum_{i=1}^{N} l_i(f(x_i) + \n",
    "\\epsilon_i) \\sum_{j=1}^{N} l_j(f(x_j) + \\epsilon_j) \\right ]\n",
    " = E_{Y,X} \\left [ \\sum_{ij} l_i l_j (f(x_i) + \\epsilon_i)(f(x_j) + \\epsilon_j) \\right ]$$\n",
    "\n",
    "$$ = E_{Y,X} \\left [\\sum_{ij} l_i l_j f(x_i) f(x_j) \\right ] + \n",
    "E_{Y,X} \\left [ \\sum_{ij} l_i l_j \\epsilon_i f(x_j) \\right ] + \n",
    "E_{Y,X} \\left [ \\sum_{ij} l_i l_j \\epsilon_j f(x_i) \\right ] + \n",
    "E_{Y,X} \\left [ \\sum_{ij} l_i l_j \\epsilon_i \\epsilon_j \\right ] $$\n",
    "\n",
    "$$ = E_{Y,X} \\left [\\sum_{ij} l_i l_j f(x_i) f(x_j) \\right ] + \n",
    "E_{Y,X} \\left [ \\sum_{ij} l_i l_j \\epsilon_i \\epsilon_j \\right ] $$ \n",
    "\n",
    "Consider:\n",
    "$$ E_{Y,X} \\left [ \\sum_{ij} l_i l_j \\epsilon_i \\epsilon_j \\right ] = \n",
    "\\sum_{ij} E_{Y,X} \\left [ l_i l_j \\right ] E_{Y,X} \\left [ \\epsilon_i \\epsilon_j \\right ] =\n",
    "\\sum_{i}^{N} E_{Y,X} \\left [ l_i^2 \\right ] E_{Y,X} \\left [ \\epsilon_i^2 \\right ] +\n",
    "\\sum_{i \\neq j} E_{Y,X} \\left [ l_i l_j \\right ] E_{Y,X} \\left [ \\epsilon_i \\epsilon_j \\right ]$$\n",
    "$$= \\sum_{i}^{N} E_{Y,X} \\left [ l_i^2 \\right ] E_{Y,X} \\left [ \\epsilon_i^2 \\right ] +\n",
    "\\sum_{i \\neq j} E_{Y,X} \\left [ l_i l_j \\right ] E_{Y,X} [ \\epsilon_i ] E_{Y,X} [\\epsilon_j ] = \n",
    "\\sum_{i}^{N} E_{Y,X} \\left [ l_i^2 \\right ] E_{Y,X} \\left [ \\epsilon_i^2 \\right ]$$\n",
    "\n",
    "$$ = \\sum_{i}^{N} E_{Y,X} \\left [ l_i^2 \\right ](Var(\\epsilon_i) - E_{Y,X}^2 [ \\epsilon_i ]) =\n",
    "\\sigma^2 E_{Y,X} \\left [ \\sum_{i}^{N} l_i^2 \\right ]$$ \n",
    "\n",
    "Thus:\n",
    "$$ E_{Y, X} \\left [ \\hat{f}^{2}(x_0) \\right ] = \n",
    "E_{Y, X} \\left [ \\sum_{ij} l_i l_j f(x_i) f(x_j) \\right ] + \n",
    "\\sigma^2 E_{Y, X} \\left [ \\sum_{i=1}^{N} l_i^2 \\right ] $$\n",
    "\n",
    "Then\n",
    "\n",
    "$$ bias = f(x_0) - E_{Y, X} [\\hat{f}(x_0)] = f(x_0) - E_{Y,X} \\left [ \\sum_{i=1}^{N} l_if(x_i) \\right ] $$\n",
    "\n",
    "And\n",
    "$$ Var(\\hat{f}(x_0)) = \n",
    "E_{Y, X} \\left [ \\hat{f}^{2}(x_0) \\right ] - E_{Y, X} [\\hat{f}(x_0)] =\n",
    "E_{Y, X} \\left [ \\sum_{ij} l_i l_j f(x_i) f(x_j) \\right ] +\n",
    "\\sigma^2 E_{Y, X} \\left [ \\sum_{i=1}^{N} l_i^2 \\right ] - \n",
    "E_{Y,X}^2 \\left [ \\sum_{i=1}^{N} l_if(x_i) \\right ] $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(d)** TO DO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<font color=#F08080>Ex. 2.8</font>** Compare the classification performance of linear regression and k–nearest neighbor classification on the zipcode data. In particular, consider only the 2’s and 3’s, and k = 1, 3, 5, 7 and 15. Show both the training and test error for each choice. The zipcode data are available from [the book website](http://www-stat.stanford.edu/ElemStatLearn).\n",
    "\n",
    "**<font color=#F08080>Solution:</font>** TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<font color=#F08080>Ex. 2.9</font>** Consider a linear regression model with p parameters, fit by least squares to a set of training data $(x_1, y_1), . . . , (x_N, y_N)$ drawn at random from a population. Let\n",
    "$ \\hat{\\beta}$ be the least squares estimate. Suppose we have some test data $(\\tilde{x}_1, \\tilde{y}_1), . . . , (\\tilde{x}_M, \\tilde{y}_M)$ drawn at random from the same pop- ulation as the training data. If $ R_{tr}(\\beta) = \\frac{1}{N} \\sum_{1}^{N} (y_i - \\beta^{T} x_i)^2$ and $ R_{te}(\\beta) = \\frac{1}{M} \\sum_{1}^{M} (\\tilde{y}_i - \\beta^{T} \\tilde{x}_i)^2$, prove that\n",
    "\n",
    "$$ E \\left [ R_{tr}(\\hat{\\beta}) \\right ] \\leq E \\left [ R_{te}(\\hat{\\beta}) \\right ], $$\n",
    "\n",
    "where the expectations are over all that is random in each expression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<font color=#F08080>Solution:</font>**\n",
    "\n",
    "Recall that $ y_i = \\beta^T x_i + \\epsilon_i$ because this is a linear regression problem.\n",
    "Since $ \\hat{\\beta}$ is estimated on the training data, we have\n",
    "\n",
    "$$ y_i = \\hat{\\beta}^T x_i + \\epsilon_i, \\forall i \\in {1, ..., N}$$\n",
    "\n",
    "So\n",
    "\n",
    "$$ E \\left [ R_{tr}(\\hat{\\beta}) \\right ] =\n",
    "E \\left [ \\frac{1}{N} \\sum_{1}^{N} (y_i - \\hat{\\beta}^{T} x_i)^2 \\right ] =\n",
    "E \\left [ \\frac{1}{N} \\sum_{1}^{N} \\epsilon_i^2 \\right ] =\n",
    "\\sigma^2, $$\n",
    "where $\\sigma$ is the variance of $\\epsilon$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say ${\\hat{\\beta}}'$ is the estimate on the test set. ${\\hat{\\beta}}'$ could be equal to $\\hat{\\beta}$ or not. The expected residual error on the test set is given by\n",
    "\n",
    "$$ E \\left [ R_{te}(\\hat{\\beta}) \\right ] =\n",
    "E \\left [ \\frac{1}{M} \\sum_{1}^{M} (y_i - {\\hat{\\beta}}'^{T} x_i + {\\hat{\\beta}}'^{T} x_i - \\hat{\\beta}^{T} x_i)^2 \\right ] =\n",
    "E \\left [ \\frac{1}{M} \\sum_{1}^{M} (\\epsilon_i + ({\\hat{\\beta}}' - \\hat{\\beta})^T x_i)^2 \\right ] =\n",
    "E \\left [ \\frac{1}{M} \\sum_{1}^{M} (\\epsilon_i + \\Delta_{\\hat{\\beta}}^T x_i)^2 \\right ] $$\n",
    "\n",
    "$$ = \\sigma^2 + E \\left [ \\frac{1}{M} \\sum_{1}^{M} (\\Delta_{\\hat{\\beta}}^T x_i)^2 \\right ] > \\sigma^2 $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
